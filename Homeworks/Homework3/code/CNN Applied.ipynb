{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from torch import optim\n",
    "from cnn import CNN\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data tools\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer, fix_length=30)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define datasets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_val_fields = [('Label', LABEL),('Text', TEXT)]\n",
    "train_set, val_set, test_set = data.TabularDataset.splits(path='data', \n",
    "    format='tsv', \n",
    "    train='train.tsv', \n",
    "    validation='dev.tsv',\n",
    "    test='test.tsv',\n",
    "    fields=train_val_fields, \n",
    "    skip_header=True)\n",
    "\n",
    "unlabelled_fields = [('id', None),('Text', TEXT)]\n",
    "unlabelled_set = data.TabularDataset(path='data/unlabelled.tsv', \n",
    "    format='tsv', \n",
    "    fields=unlabelled_fields, \n",
    "    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build vocab\n",
    "TEXT.build_vocab(train_set, max_size=100000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, H, conv_size, out_channels, pretrained_vocab=None, maxPool=False):\n",
    "        super(CNN, self).__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size,H)\n",
    "        self.conv1d = nn.Conv1d(H, out_channels, conv_size)\n",
    "        if maxPool:\n",
    "            self.pool = nn.AdaptiveMaxPool2d((out_channels, 1))\n",
    "        else:\n",
    "            self.pool = nn.AdaptiveAvgPool2d((out_channels, 1))\n",
    "        self.linear = nn.Linear(out_channels, 2)\n",
    "        \n",
    "        if pretrained_vocab:\n",
    "            self.embedding.weight.data.copy_(pretrained_vocab.vectors)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x).permute(1,2,0)\n",
    "        h_conv1d = self.conv1d(h_embedding)\n",
    "        h_pool = self.pool(h_conv1d).clamp(min=0).squeeze(2)\n",
    "        h_linear = self.linear(h_pool)\n",
    "        logits = F.softmax(h_linear, dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get data iterators\n",
    "train_it_64, val_it_64 = data.Iterator.splits(\n",
    "        (train_set, val_set), sort_key=lambda x: len(x.Text),\n",
    "        batch_size=64, device=device)\n",
    "\n",
    "train_it, val_it = data.Iterator.splits(\n",
    "        (train_set, val_set), sort_key=lambda x: len(x.Text),\n",
    "        batch_size=4, device=device)\n",
    "test_it = data.BucketIterator(\n",
    "    dataset=test_set,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.Text),\n",
    "    shuffle=False)\n",
    "unlabelled_it = data.BucketIterator(\n",
    "    dataset=unlabelled_set,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.Text),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_it,\n",
    "    'val': val_it,\n",
    "    'test': test_it,\n",
    "    'unlabelled': unlabelled_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helpers\n",
    "def forward(model, batch, criterion, multiclass=True):\n",
    "    if multiclass:\n",
    "            outputs = model(batch.Text)\n",
    "            _,preds = torch.max(outputs,1)\n",
    "            loss = criterion(outputs, batch.Label)\n",
    "\n",
    "            correct = (preds == batch.Label).float()\n",
    "            acc = correct.sum()/len(correct)\n",
    "    else:\n",
    "        outputs = model(batch.Text).squeeze(1)\n",
    "        loss = criterion(outputs, batch.Label.float())\n",
    "\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        correct = (preds == batch.Label.float()).float()\n",
    "        acc = correct.sum()/len(correct)\n",
    "    return loss, acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, multiclass=True):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        loss, acc = forward(model, batch, criterion, multiclass=multiclass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, multiclass=True):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            loss, acc = forward(model, batch, criterion, multiclass=multiclass)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def run_model(device, dataloaders, model, optimizer, criterion, num_epochs, multiclass=True):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    best_model_params = None\n",
    "    \n",
    "    train_it, val_it, test_it = dataloaders['train'], dataloaders['val'], dataloaders['test']\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, train_it, optimizer, criterion, multiclass=multiclass)\n",
    "        valid_loss, valid_acc = evaluate(model, val_it, criterion, multiclass=multiclass)\n",
    "        \n",
    "        if valid_acc > best_val_acc:\n",
    "            best_val_acc = valid_acc\n",
    "            best_model_params = model.parameters()\n",
    "\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "\n",
    "    model\n",
    "    test_loss, test_acc = evaluate(model, test_it, criterion, multiclass=multiclass)\n",
    "    print('Test loss:', test_loss)\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.502 | Train Acc: 79.84% | Val. Loss: 0.435 | Val. Acc: 87.28% |\n",
      "| Epoch: 02 | Train Loss: 0.430 | Train Acc: 87.81% | Val. Loss: 0.397 | Val. Acc: 91.23% |\n",
      "| Epoch: 03 | Train Loss: 0.410 | Train Acc: 89.95% | Val. Loss: 0.387 | Val. Acc: 92.32% |\n",
      "| Epoch: 04 | Train Loss: 0.402 | Train Acc: 90.63% | Val. Loss: 0.405 | Val. Acc: 90.61% |\n",
      "| Epoch: 05 | Train Loss: 0.394 | Train Acc: 91.62% | Val. Loss: 0.380 | Val. Acc: 93.14% |\n",
      "| Epoch: 06 | Train Loss: 0.389 | Train Acc: 92.12% | Val. Loss: 0.383 | Val. Acc: 92.70% |\n",
      "| Epoch: 07 | Train Loss: 0.386 | Train Acc: 92.42% | Val. Loss: 0.380 | Val. Acc: 93.06% |\n",
      "| Epoch: 08 | Train Loss: 0.383 | Train Acc: 92.73% | Val. Loss: 0.377 | Val. Acc: 93.31% |\n",
      "| Epoch: 09 | Train Loss: 0.380 | Train Acc: 93.09% | Val. Loss: 0.382 | Val. Acc: 92.87% |\n",
      "| Epoch: 10 | Train Loss: 0.377 | Train Acc: 93.44% | Val. Loss: 0.394 | Val. Acc: 91.63% |\n",
      "| Epoch: 11 | Train Loss: 0.375 | Train Acc: 93.58% | Val. Loss: 0.393 | Val. Acc: 91.66% |\n",
      "| Epoch: 12 | Train Loss: 0.373 | Train Acc: 93.77% | Val. Loss: 0.385 | Val. Acc: 92.57% |\n",
      "| Epoch: 13 | Train Loss: 0.373 | Train Acc: 93.85% | Val. Loss: 0.377 | Val. Acc: 93.43% |\n",
      "| Epoch: 14 | Train Loss: 0.371 | Train Acc: 94.00% | Val. Loss: 0.413 | Val. Acc: 89.71% |\n",
      "| Epoch: 15 | Train Loss: 0.370 | Train Acc: 94.21% | Val. Loss: 0.387 | Val. Acc: 92.49% |\n",
      "| Epoch: 16 | Train Loss: 0.368 | Train Acc: 94.42% | Val. Loss: 0.382 | Val. Acc: 93.03% |\n",
      "| Epoch: 17 | Train Loss: 0.367 | Train Acc: 94.50% | Val. Loss: 0.377 | Val. Acc: 93.46% |\n",
      "| Epoch: 18 | Train Loss: 0.366 | Train Acc: 94.60% | Val. Loss: 0.375 | Val. Acc: 93.57% |\n",
      "| Epoch: 19 | Train Loss: 0.366 | Train Acc: 94.64% | Val. Loss: 0.378 | Val. Acc: 93.35% |\n",
      "| Epoch: 20 | Train Loss: 0.367 | Train Acc: 94.45% | Val. Loss: 0.378 | Val. Acc: 93.27% |\n",
      "| Epoch: 21 | Train Loss: 0.368 | Train Acc: 94.37% | Val. Loss: 0.379 | Val. Acc: 93.27% |\n",
      "| Epoch: 22 | Train Loss: 0.368 | Train Acc: 94.42% | Val. Loss: 0.372 | Val. Acc: 93.95% |\n",
      "| Epoch: 23 | Train Loss: 0.365 | Train Acc: 94.65% | Val. Loss: 0.374 | Val. Acc: 93.72% |\n",
      "| Epoch: 24 | Train Loss: 0.363 | Train Acc: 94.81% | Val. Loss: 0.376 | Val. Acc: 93.49% |\n",
      "| Epoch: 25 | Train Loss: 0.363 | Train Acc: 94.84% | Val. Loss: 0.371 | Val. Acc: 94.10% |\n",
      "Test loss: 0.3762234656989574\n",
      "Test accuracy: 0.9355\n"
     ]
    }
   ],
   "source": [
    "### Train CNN Model kernel size 5 (with pretrain)\n",
    "model = CNN(len(TEXT.vocab), 100, 5, 128, pretrained_vocab=TEXT.vocab)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_5_model = run_model(device, dataloaders, model, optimizer, criterion, 25, multiclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.477 | Train Acc: 82.47% | Val. Loss: 0.388 | Val. Acc: 92.30% |\n",
      "| Epoch: 02 | Train Loss: 0.407 | Train Acc: 90.20% | Val. Loss: 0.392 | Val. Acc: 91.83% |\n",
      "| Epoch: 03 | Train Loss: 0.391 | Train Acc: 91.84% | Val. Loss: 0.377 | Val. Acc: 93.37% |\n",
      "| Epoch: 04 | Train Loss: 0.384 | Train Acc: 92.64% | Val. Loss: 0.378 | Val. Acc: 93.35% |\n",
      "| Epoch: 05 | Train Loss: 0.379 | Train Acc: 93.16% | Val. Loss: 0.376 | Val. Acc: 93.38% |\n",
      "| Epoch: 06 | Train Loss: 0.375 | Train Acc: 93.56% | Val. Loss: 0.413 | Val. Acc: 89.61% |\n",
      "| Epoch: 07 | Train Loss: 0.372 | Train Acc: 93.88% | Val. Loss: 0.366 | Val. Acc: 94.56% |\n",
      "| Epoch: 08 | Train Loss: 0.369 | Train Acc: 94.28% | Val. Loss: 0.372 | Val. Acc: 93.84% |\n",
      "| Epoch: 09 | Train Loss: 0.367 | Train Acc: 94.44% | Val. Loss: 0.367 | Val. Acc: 94.44% |\n",
      "| Epoch: 10 | Train Loss: 0.364 | Train Acc: 94.72% | Val. Loss: 0.378 | Val. Acc: 93.38% |\n",
      "| Epoch: 11 | Train Loss: 0.363 | Train Acc: 94.88% | Val. Loss: 0.365 | Val. Acc: 94.58% |\n",
      "| Epoch: 12 | Train Loss: 0.362 | Train Acc: 95.02% | Val. Loss: 0.387 | Val. Acc: 92.38% |\n",
      "| Epoch: 13 | Train Loss: 0.361 | Train Acc: 95.04% | Val. Loss: 0.366 | Val. Acc: 94.56% |\n",
      "| Epoch: 14 | Train Loss: 0.360 | Train Acc: 95.20% | Val. Loss: 0.366 | Val. Acc: 94.55% |\n",
      "| Epoch: 15 | Train Loss: 0.359 | Train Acc: 95.21% | Val. Loss: 0.370 | Val. Acc: 94.15% |\n",
      "| Epoch: 16 | Train Loss: 0.358 | Train Acc: 95.36% | Val. Loss: 0.365 | Val. Acc: 94.60% |\n",
      "| Epoch: 17 | Train Loss: 0.358 | Train Acc: 95.45% | Val. Loss: 0.366 | Val. Acc: 94.61% |\n",
      "| Epoch: 18 | Train Loss: 0.357 | Train Acc: 95.49% | Val. Loss: 0.363 | Val. Acc: 94.89% |\n",
      "| Epoch: 19 | Train Loss: 0.355 | Train Acc: 95.70% | Val. Loss: 0.368 | Val. Acc: 94.47% |\n",
      "| Epoch: 20 | Train Loss: 0.355 | Train Acc: 95.71% | Val. Loss: 0.363 | Val. Acc: 94.90% |\n",
      "Test loss: 0.3698499638020992\n",
      "Test accuracy: 0.942\n"
     ]
    }
   ],
   "source": [
    "### Train CNN Model kernel size 7 (with pretrain)\n",
    "model = CNN(len(TEXT.vocab), 100, 7, 128, pretrained_vocab=TEXT.vocab)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_model = run_model(device, dataloaders, model, optimizer, criterion, 20, multiclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.414 | Train Acc: 89.48% | Val. Loss: 0.387 | Val. Acc: 92.33% |\n",
      "| Epoch: 02 | Train Loss: 0.379 | Train Acc: 93.23% | Val. Loss: 0.395 | Val. Acc: 91.52% |\n",
      "| Epoch: 03 | Train Loss: 0.367 | Train Acc: 94.44% | Val. Loss: 0.362 | Val. Acc: 95.00% |\n",
      "| Epoch: 04 | Train Loss: 0.363 | Train Acc: 94.82% | Val. Loss: 0.361 | Val. Acc: 95.15% |\n",
      "| Epoch: 05 | Train Loss: 0.361 | Train Acc: 95.13% | Val. Loss: 0.383 | Val. Acc: 92.81% |\n",
      "| Epoch: 06 | Train Loss: 0.357 | Train Acc: 95.53% | Val. Loss: 0.360 | Val. Acc: 95.28% |\n",
      "| Epoch: 07 | Train Loss: 0.355 | Train Acc: 95.74% | Val. Loss: 0.360 | Val. Acc: 95.27% |\n",
      "| Epoch: 08 | Train Loss: 0.354 | Train Acc: 95.86% | Val. Loss: 0.359 | Val. Acc: 95.32% |\n",
      "| Epoch: 09 | Train Loss: 0.351 | Train Acc: 96.16% | Val. Loss: 0.358 | Val. Acc: 95.39% |\n",
      "| Epoch: 10 | Train Loss: 0.350 | Train Acc: 96.22% | Val. Loss: 0.359 | Val. Acc: 95.34% |\n",
      "Test loss: 0.3615973283946514\n",
      "Test accuracy: 0.9512\n"
     ]
    }
   ],
   "source": [
    "### Train CNN Model Max Pool kernel size 5 (with pretrain)\n",
    "model = CNN(len(TEXT.vocab), 100, 5, 128, pretrained_vocab=TEXT.vocab, maxPool=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_model_max_5 = run_model(device, dataloaders, model, optimizer, criterion, 10, multiclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.419 | Train Acc: 88.96% | Val. Loss: 0.411 | Val. Acc: 89.83% |\n",
      "| Epoch: 02 | Train Loss: 0.381 | Train Acc: 92.99% | Val. Loss: 0.373 | Val. Acc: 93.83% |\n",
      "| Epoch: 03 | Train Loss: 0.371 | Train Acc: 94.10% | Val. Loss: 0.379 | Val. Acc: 93.28% |\n",
      "| Epoch: 04 | Train Loss: 0.366 | Train Acc: 94.59% | Val. Loss: 0.364 | Val. Acc: 94.71% |\n",
      "| Epoch: 05 | Train Loss: 0.366 | Train Acc: 94.55% | Val. Loss: 0.373 | Val. Acc: 93.91% |\n",
      "| Epoch: 06 | Train Loss: 0.360 | Train Acc: 95.25% | Val. Loss: 0.362 | Val. Acc: 95.06% |\n",
      "| Epoch: 07 | Train Loss: 0.358 | Train Acc: 95.41% | Val. Loss: 0.361 | Val. Acc: 95.16% |\n",
      "| Epoch: 08 | Train Loss: 0.357 | Train Acc: 95.55% | Val. Loss: 0.362 | Val. Acc: 95.01% |\n",
      "| Epoch: 09 | Train Loss: 0.354 | Train Acc: 95.81% | Val. Loss: 0.358 | Val. Acc: 95.52% |\n",
      "| Epoch: 10 | Train Loss: 0.354 | Train Acc: 95.85% | Val. Loss: 0.363 | Val. Acc: 94.97% |\n",
      "Test loss: 0.36499896264672277\n",
      "Test accuracy: 0.9473\n"
     ]
    }
   ],
   "source": [
    "### Train CNN Model Max Pool kernel size 5 (with pretrain)\n",
    "model = CNN(len(TEXT.vocab), 100, 7, 128, pretrained_vocab=TEXT.vocab, maxPool=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_model_max_7 = run_model(device, dataloaders, model, optimizer, criterion, 10, multiclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-611d9d8e9920>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-611d9d8e9920>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Test loss: 0.38745442863106727\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Test loss: 0.38745442863106727\n",
    "Test accuracy: 0.9254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eecs445)",
   "language": "python",
   "name": "eecs445"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
