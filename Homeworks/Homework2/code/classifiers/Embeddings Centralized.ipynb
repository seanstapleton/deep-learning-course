{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data tools\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define datasets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_val_fields = [('Label', LABEL),('Text', TEXT)]\n",
    "train_set, val_set, test_set = data.TabularDataset.splits(path='../data', \n",
    "    format='tsv', \n",
    "    train='train.tsv', \n",
    "    validation='dev.tsv',\n",
    "    test='test.tsv',\n",
    "    fields=train_val_fields, \n",
    "    skip_header=True)\n",
    "\n",
    "unlabelled_fields = [('id', None),('Text', TEXT)]\n",
    "unlabelled_set = data.TabularDataset(path='../data/unlabelled.tsv', \n",
    "    format='tsv', \n",
    "    fields=unlabelled_fields, \n",
    "    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build vocab\n",
    "TEXT.build_vocab(train_set, max_size=100000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get data iterators\n",
    "train_it, val_it = data.Iterator.splits(\n",
    "        (train_set, val_set), sort_key=lambda x: len(x.Text),\n",
    "        batch_size=64, device=device)\n",
    "test_it = data.BucketIterator(\n",
    "    dataset=test_set,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.Text),\n",
    "    shuffle=False)\n",
    "unlabelled_it = data.BucketIterator(\n",
    "    dataset=unlabelled_set,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.Text),\n",
    "    shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_it,\n",
    "    'val': val_it,\n",
    "    'test': test_it,\n",
    "    'unlabelled': unlabelled_it\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classifiers\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "\n",
    "    # Initialize the classifier\n",
    "    def __init__(self, emb_dim, num_labels, vocab_size, pretrained_vocab=None):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, num_labels)\n",
    "        \n",
    "        if pretrained_vocab:\n",
    "            self.embedding.weight.data.copy_(pretrained_vocab.vectors)\n",
    "        \n",
    "    def forward(self, inputs):        \n",
    "        z1 = self.embedding(inputs).permute(1,0,2)\n",
    "        z2 = F.avg_pool2d(z1, (z1.shape[1], 1)).squeeze(1) \n",
    "        out = self.linear(z2)\n",
    "        return torch.sigmoid(out)\n",
    "    \n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_labels, hidden_dim, vocab_size, pretrained_vocab=None):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        \n",
    "        if pretrained_vocab:\n",
    "            self.embedding.weight.data.copy_(pretrained_vocab.vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        z1 = self.embedding(x)        \n",
    "        z2, h2 = self.rnn(z1)\n",
    "        z3 = self.linear(h2.squeeze(0))\n",
    "        \n",
    "        return z3\n",
    "    \n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_labels, hidden_dim, vocab_size, pretrained_vocab=None):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        \n",
    "        if pretrained_vocab:\n",
    "            self.embedding.weight.data.copy_(pretrained_vocab.vectors)\n",
    "\n",
    "    def forward(self, x, batch_size=None):\n",
    "        z1 = self.embedding(x)\n",
    "        z2, (h2, c2) = self.lstm(z1)\n",
    "        z3 = self.linear(h2[-1])\n",
    "\n",
    "        return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helpers\n",
    "def forward(model, batch, criterion, multiclass=True):\n",
    "    if multiclass:\n",
    "            outputs = model(batch.Text).squeeze(1)\n",
    "            _,preds = torch.max(outputs,1)\n",
    "            loss = criterion(outputs, batch.Label)\n",
    "\n",
    "            correct = (preds == batch.Label).float()\n",
    "            acc = correct.sum()/len(correct)\n",
    "    else:\n",
    "        outputs = model(batch.Text).squeeze(1)\n",
    "        loss = criterion(outputs, batch.Label.float())\n",
    "\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        correct = (preds == batch.Label.float()).float()\n",
    "        acc = correct.sum()/len(correct)\n",
    "    return loss, acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, multiclass=True):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        loss, acc = forward(model, batch, criterion, multiclass=multiclass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, multiclass=True):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            loss, acc = forward(model, batch, criterion, multiclass=multiclass)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(device, dataloaders, model, optimizer, criterion, num_epochs, multiclass=True):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_it, val_it, test_it = dataloaders['train'], dataloaders['val'], dataloaders['test']\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, train_it, optimizer, criterion, multiclass=multiclass)\n",
    "        valid_loss, valid_acc = evaluate(model, val_it, criterion, multiclass=multiclass)\n",
    "\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_it, criterion, multiclass=multiclass)\n",
    "    print('Test loss:', test_loss)\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.667 | Train Acc: 61.93% | Val. Loss: 0.644 | Val. Acc: 62.51% |\n",
      "| Epoch: 02 | Train Loss: 0.635 | Train Acc: 68.96% | Val. Loss: 0.619 | Val. Acc: 66.26% |\n",
      "| Epoch: 03 | Train Loss: 0.616 | Train Acc: 71.05% | Val. Loss: 0.602 | Val. Acc: 68.80% |\n",
      "| Epoch: 04 | Train Loss: 0.603 | Train Acc: 72.33% | Val. Loss: 0.588 | Val. Acc: 70.71% |\n",
      "| Epoch: 05 | Train Loss: 0.592 | Train Acc: 73.03% | Val. Loss: 0.579 | Val. Acc: 71.93% |\n",
      "| Epoch: 06 | Train Loss: 0.584 | Train Acc: 73.86% | Val. Loss: 0.570 | Val. Acc: 72.75% |\n",
      "| Epoch: 07 | Train Loss: 0.577 | Train Acc: 74.36% | Val. Loss: 0.563 | Val. Acc: 73.59% |\n",
      "| Epoch: 08 | Train Loss: 0.570 | Train Acc: 74.90% | Val. Loss: 0.557 | Val. Acc: 74.09% |\n",
      "| Epoch: 09 | Train Loss: 0.564 | Train Acc: 75.22% | Val. Loss: 0.551 | Val. Acc: 74.81% |\n",
      "| Epoch: 10 | Train Loss: 0.559 | Train Acc: 75.73% | Val. Loss: 0.546 | Val. Acc: 75.57% |\n",
      "| Epoch: 11 | Train Loss: 0.554 | Train Acc: 76.19% | Val. Loss: 0.541 | Val. Acc: 76.00% |\n",
      "| Epoch: 12 | Train Loss: 0.550 | Train Acc: 76.51% | Val. Loss: 0.537 | Val. Acc: 76.39% |\n",
      "| Epoch: 13 | Train Loss: 0.546 | Train Acc: 76.85% | Val. Loss: 0.533 | Val. Acc: 76.81% |\n",
      "| Epoch: 14 | Train Loss: 0.542 | Train Acc: 77.10% | Val. Loss: 0.528 | Val. Acc: 77.27% |\n",
      "| Epoch: 15 | Train Loss: 0.539 | Train Acc: 77.56% | Val. Loss: 0.525 | Val. Acc: 77.71% |\n",
      "| Epoch: 16 | Train Loss: 0.535 | Train Acc: 77.96% | Val. Loss: 0.521 | Val. Acc: 78.04% |\n",
      "| Epoch: 17 | Train Loss: 0.532 | Train Acc: 78.19% | Val. Loss: 0.518 | Val. Acc: 78.33% |\n",
      "| Epoch: 18 | Train Loss: 0.529 | Train Acc: 78.61% | Val. Loss: 0.514 | Val. Acc: 78.87% |\n",
      "| Epoch: 19 | Train Loss: 0.526 | Train Acc: 78.85% | Val. Loss: 0.511 | Val. Acc: 79.17% |\n",
      "| Epoch: 20 | Train Loss: 0.523 | Train Acc: 79.08% | Val. Loss: 0.508 | Val. Acc: 79.54% |\n",
      "| Epoch: 21 | Train Loss: 0.520 | Train Acc: 79.45% | Val. Loss: 0.505 | Val. Acc: 79.81% |\n",
      "| Epoch: 22 | Train Loss: 0.517 | Train Acc: 79.68% | Val. Loss: 0.502 | Val. Acc: 80.03% |\n",
      "| Epoch: 23 | Train Loss: 0.515 | Train Acc: 79.90% | Val. Loss: 0.500 | Val. Acc: 80.37% |\n",
      "| Epoch: 24 | Train Loss: 0.512 | Train Acc: 80.22% | Val. Loss: 0.497 | Val. Acc: 80.63% |\n",
      "| Epoch: 25 | Train Loss: 0.510 | Train Acc: 80.48% | Val. Loss: 0.494 | Val. Acc: 80.86% |\n",
      "Test loss: 0.4908353900372982\n",
      "Test accuracy: 0.8112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingClassifier(\n",
       "  (embedding): Embedding(7507, 100)\n",
       "  (linear): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train Embedding Model (no pretrain)\n",
    "model = EmbeddingClassifier(100, 2, len(TEXT.vocab))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "run_model(device, dataloaders, model, optimizer, criterion, 25, multiclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.674 | Train Acc: 62.26% | Val. Loss: 0.657 | Val. Acc: 56.47% |\n",
      "| Epoch: 02 | Train Loss: 0.644 | Train Acc: 68.72% | Val. Loss: 0.637 | Val. Acc: 61.39% |\n",
      "| Epoch: 03 | Train Loss: 0.623 | Train Acc: 70.16% | Val. Loss: 0.615 | Val. Acc: 65.98% |\n",
      "| Epoch: 04 | Train Loss: 0.606 | Train Acc: 73.15% | Val. Loss: 0.579 | Val. Acc: 73.89% |\n",
      "| Epoch: 05 | Train Loss: 0.590 | Train Acc: 75.80% | Val. Loss: 0.556 | Val. Acc: 76.83% |\n",
      "| Epoch: 06 | Train Loss: 0.576 | Train Acc: 77.67% | Val. Loss: 0.540 | Val. Acc: 78.34% |\n",
      "| Epoch: 07 | Train Loss: 0.562 | Train Acc: 79.45% | Val. Loss: 0.522 | Val. Acc: 80.47% |\n",
      "| Epoch: 08 | Train Loss: 0.550 | Train Acc: 80.76% | Val. Loss: 0.510 | Val. Acc: 81.64% |\n",
      "| Epoch: 09 | Train Loss: 0.540 | Train Acc: 81.84% | Val. Loss: 0.502 | Val. Acc: 82.10% |\n",
      "| Epoch: 10 | Train Loss: 0.531 | Train Acc: 82.64% | Val. Loss: 0.494 | Val. Acc: 82.79% |\n",
      "| Epoch: 11 | Train Loss: 0.522 | Train Acc: 83.29% | Val. Loss: 0.488 | Val. Acc: 83.25% |\n",
      "| Epoch: 12 | Train Loss: 0.515 | Train Acc: 83.92% | Val. Loss: 0.480 | Val. Acc: 84.32% |\n",
      "| Epoch: 13 | Train Loss: 0.508 | Train Acc: 84.43% | Val. Loss: 0.474 | Val. Acc: 84.75% |\n",
      "| Epoch: 14 | Train Loss: 0.502 | Train Acc: 84.95% | Val. Loss: 0.468 | Val. Acc: 85.36% |\n",
      "| Epoch: 15 | Train Loss: 0.496 | Train Acc: 85.40% | Val. Loss: 0.464 | Val. Acc: 85.67% |\n",
      "| Epoch: 16 | Train Loss: 0.491 | Train Acc: 85.84% | Val. Loss: 0.460 | Val. Acc: 86.00% |\n",
      "| Epoch: 17 | Train Loss: 0.487 | Train Acc: 86.32% | Val. Loss: 0.455 | Val. Acc: 86.33% |\n",
      "| Epoch: 18 | Train Loss: 0.482 | Train Acc: 86.71% | Val. Loss: 0.452 | Val. Acc: 86.55% |\n",
      "| Epoch: 19 | Train Loss: 0.478 | Train Acc: 87.04% | Val. Loss: 0.448 | Val. Acc: 86.85% |\n",
      "| Epoch: 20 | Train Loss: 0.474 | Train Acc: 87.41% | Val. Loss: 0.443 | Val. Acc: 87.54% |\n",
      "| Epoch: 21 | Train Loss: 0.470 | Train Acc: 87.71% | Val. Loss: 0.440 | Val. Acc: 87.76% |\n",
      "| Epoch: 22 | Train Loss: 0.467 | Train Acc: 87.97% | Val. Loss: 0.436 | Val. Acc: 88.23% |\n",
      "| Epoch: 23 | Train Loss: 0.463 | Train Acc: 88.28% | Val. Loss: 0.434 | Val. Acc: 88.57% |\n",
      "| Epoch: 24 | Train Loss: 0.460 | Train Acc: 88.58% | Val. Loss: 0.432 | Val. Acc: 88.77% |\n",
      "| Epoch: 25 | Train Loss: 0.457 | Train Acc: 88.76% | Val. Loss: 0.428 | Val. Acc: 89.24% |\n",
      "Test loss: 0.4273523548722267\n",
      "Test accuracy: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingClassifier(\n",
       "  (embedding): Embedding(7507, 100)\n",
       "  (linear): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train Embedding Model (with pretrain)\n",
    "model = EmbeddingClassifier(100, 2, len(TEXT.vocab), pretrained_vocab=TEXT.vocab)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "run_model(device, dataloaders, model, optimizer, criterion, 25, multiclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.258 | Train Acc: 89.75% | Val. Loss: 0.165 | Val. Acc: 95.13% |\n",
      "| Epoch: 02 | Train Loss: 0.144 | Train Acc: 95.15% | Val. Loss: 0.127 | Val. Acc: 95.50% |\n",
      "| Epoch: 03 | Train Loss: 0.121 | Train Acc: 96.14% | Val. Loss: 0.121 | Val. Acc: 95.91% |\n",
      "| Epoch: 04 | Train Loss: 0.107 | Train Acc: 96.52% | Val. Loss: 0.129 | Val. Acc: 95.48% |\n",
      "| Epoch: 05 | Train Loss: 0.097 | Train Acc: 96.87% | Val. Loss: 0.131 | Val. Acc: 95.47% |\n",
      "Test loss: 0.1405945674168499\n",
      "Test accuracy: 0.951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embedding): Embedding(7507, 100)\n",
       "  (rnn): RNN(100, 300)\n",
       "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train RNN Model (with pretrain)\n",
    "model = RNNClassifier(100, 1, 300, len(TEXT.vocab), pretrained_vocab=TEXT.vocab)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "run_model(device, dataloaders, model, optimizer, criterion, 5, multiclass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.219 | Train Acc: 90.83% | Val. Loss: 0.181 | Val. Acc: 95.28% |\n",
      "| Epoch: 02 | Train Loss: 0.100 | Train Acc: 96.53% | Val. Loss: 0.124 | Val. Acc: 96.66% |\n",
      "| Epoch: 03 | Train Loss: 0.072 | Train Acc: 97.61% | Val. Loss: 0.102 | Val. Acc: 96.91% |\n",
      "| Epoch: 04 | Train Loss: 0.053 | Train Acc: 98.20% | Val. Loss: 0.106 | Val. Acc: 96.77% |\n",
      "| Epoch: 05 | Train Loss: 0.041 | Train Acc: 98.72% | Val. Loss: 0.104 | Val. Acc: 96.60% |\n",
      "Test loss: 0.11096419042692068\n",
      "Test accuracy: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(7507, 100)\n",
       "  (lstm): LSTM(100, 300)\n",
       "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train LSTM Model (with pretrain)\n",
    "model = LSTMClassifier(100, 1, 300, len(TEXT.vocab), pretrained_vocab=TEXT.vocab)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "run_model(device, dataloaders, model, optimizer, criterion, 5, multiclass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
