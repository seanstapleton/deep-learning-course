{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "train_val_fields = [('Label', LABEL),('Text', TEXT)]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = data.TabularDataset.splits(path='../data', \n",
    "    format='tsv', \n",
    "    train='train.tsv', \n",
    "    validation='dev.tsv',\n",
    "    test='test.tsv',\n",
    "    fields=train_val_fields, \n",
    "    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_fields = [('id', None),('Text', TEXT)]\n",
    "unlabelled_set = data.TabularDataset(path='../data/unlabelled.tsv', \n",
    "    format='tsv', \n",
    "    fields=unlabelled_fields, \n",
    "    skip_header=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_set, max_size=100000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train_set, val_set, test_set), sort_key=lambda x: len(x.Text),\n",
    "        batch_size=64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_it = data.BucketIterator(\n",
    "    dataset=unlabelled_set,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.Text),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TEXT.vocab\n",
    "embed = nn.Embedding(len(vocab), 100)\n",
    "embed.weight.data.copy_(vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClassifier(nn.Module):\n",
    "\n",
    "    # Initialize the classifier\n",
    "    def __init__(self, emb_dim, num_labels, vocab_size, pretrained_vocab=None):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, num_labels)\n",
    "        \n",
    "        if pretrained_vocab:\n",
    "            self.embedding.weight.data.copy_(pretrained_vocab.vectors)\n",
    "        \n",
    "    def forward(self, inputs):        \n",
    "        z1 = self.embedding(inputs).permute(1,0,2)\n",
    "        z2 = F.avg_pool2d(z1, (z1.shape[1], 1)).squeeze(1) \n",
    "        out = self.linear(z2)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_rnn(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        outputs = model(batch.Text).squeeze(1)\n",
    "        loss = criterion(outputs, batch.Label.float())\n",
    "        acc = binary_accuracy_rnn(outputs, batch.Label.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    correct = (preds == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        outputs = model(batch.Text).squeeze(1)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, batch.Label)\n",
    "        acc = binary_accuracy(preds, batch.Label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            outputs = model(batch.Text).squeeze(1)\n",
    "            loss = criterion(outputs, batch.Label.float())\n",
    "            \n",
    "            acc = binary_accuracy_rnn(outputs, batch.Label.float())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            outputs = model(batch.Text).squeeze(1)\n",
    "            _,preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, batch.Label)\n",
    "            \n",
    "            acc = binary_accuracy(preds, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_2():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    train_it, val_it, test_it = data.BucketIterator.splits(\n",
    "            datasets=(train_set, val_set, test_set),\n",
    "            batch_size=32, device=device,\n",
    "            sort_key=lambda x: len(x.Text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "    \n",
    "    ## define model\n",
    "    model = EmbeddingClassifier(100, 2, len(TEXT.vocab))\n",
    "#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(25):\n",
    "        train_loss, train_acc = train(model, train_it, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_it, criterion)\n",
    "\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_it, criterion)\n",
    "    print('Test loss:', test_loss)\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.654 | Train Acc: 63.99% | Val. Loss: 0.639 | Val. Acc: 63.46% |\n",
      "| Epoch: 02 | Train Loss: 0.616 | Train Acc: 70.01% | Val. Loss: 0.610 | Val. Acc: 68.01% |\n",
      "| Epoch: 03 | Train Loss: 0.596 | Train Acc: 72.14% | Val. Loss: 0.590 | Val. Acc: 70.55% |\n",
      "| Epoch: 04 | Train Loss: 0.581 | Train Acc: 73.52% | Val. Loss: 0.575 | Val. Acc: 72.50% |\n",
      "| Epoch: 05 | Train Loss: 0.569 | Train Acc: 74.55% | Val. Loss: 0.565 | Val. Acc: 73.64% |\n",
      "| Epoch: 06 | Train Loss: 0.559 | Train Acc: 75.48% | Val. Loss: 0.555 | Val. Acc: 74.52% |\n",
      "| Epoch: 07 | Train Loss: 0.550 | Train Acc: 76.19% | Val. Loss: 0.546 | Val. Acc: 75.36% |\n",
      "| Epoch: 08 | Train Loss: 0.543 | Train Acc: 76.93% | Val. Loss: 0.538 | Val. Acc: 76.16% |\n",
      "| Epoch: 09 | Train Loss: 0.536 | Train Acc: 77.71% | Val. Loss: 0.531 | Val. Acc: 76.61% |\n",
      "| Epoch: 10 | Train Loss: 0.529 | Train Acc: 78.40% | Val. Loss: 0.525 | Val. Acc: 77.60% |\n",
      "| Epoch: 11 | Train Loss: 0.523 | Train Acc: 78.90% | Val. Loss: 0.518 | Val. Acc: 78.17% |\n",
      "| Epoch: 12 | Train Loss: 0.517 | Train Acc: 79.65% | Val. Loss: 0.513 | Val. Acc: 78.88% |\n",
      "| Epoch: 13 | Train Loss: 0.512 | Train Acc: 80.10% | Val. Loss: 0.507 | Val. Acc: 79.51% |\n",
      "| Epoch: 14 | Train Loss: 0.507 | Train Acc: 80.71% | Val. Loss: 0.502 | Val. Acc: 80.04% |\n",
      "| Epoch: 15 | Train Loss: 0.502 | Train Acc: 81.37% | Val. Loss: 0.497 | Val. Acc: 80.47% |\n",
      "| Epoch: 16 | Train Loss: 0.498 | Train Acc: 81.77% | Val. Loss: 0.493 | Val. Acc: 80.80% |\n",
      "| Epoch: 17 | Train Loss: 0.494 | Train Acc: 82.27% | Val. Loss: 0.489 | Val. Acc: 81.35% |\n",
      "| Epoch: 18 | Train Loss: 0.490 | Train Acc: 82.68% | Val. Loss: 0.484 | Val. Acc: 81.84% |\n",
      "| Epoch: 19 | Train Loss: 0.486 | Train Acc: 83.20% | Val. Loss: 0.481 | Val. Acc: 82.33% |\n",
      "| Epoch: 20 | Train Loss: 0.482 | Train Acc: 83.48% | Val. Loss: 0.476 | Val. Acc: 82.73% |\n",
      "| Epoch: 21 | Train Loss: 0.479 | Train Acc: 83.82% | Val. Loss: 0.473 | Val. Acc: 83.13% |\n",
      "| Epoch: 22 | Train Loss: 0.476 | Train Acc: 84.18% | Val. Loss: 0.470 | Val. Acc: 83.49% |\n",
      "| Epoch: 23 | Train Loss: 0.472 | Train Acc: 84.60% | Val. Loss: 0.466 | Val. Acc: 83.92% |\n",
      "| Epoch: 24 | Train Loss: 0.469 | Train Acc: 84.85% | Val. Loss: 0.463 | Val. Acc: 84.30% |\n",
      "| Epoch: 25 | Train Loss: 0.466 | Train Acc: 85.09% | Val. Loss: 0.460 | Val. Acc: 84.52% |\n",
      "Test loss: 0.4532473308209794\n",
      "Test accuracy: 0.8563298722044729\n"
     ]
    }
   ],
   "source": [
    "model_2 = run_model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_set = data.TabularDataset(path='../data/test.tsv', \n",
    "    format='tsv', \n",
    "    fields=train_val_fields, \n",
    "    skip_header=True)\n",
    "test_test_it = data.BucketIterator(\n",
    "    dataset=test_test_set,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.Text),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_unlabelled(model, it, name):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        f = open(name, 'w')\n",
    "        for batch in it:\n",
    "            outputs = model(batch.Text).squeeze(1)\n",
    "            _,preds = torch.max(outputs,1)\n",
    "            output_string = '\\n'.join(str(s) for s in preds.numpy())\n",
    "            f.write(output_string+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_unlabelled(model_2, test_test_it, 'TEST_OUT.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, labelled=True):\n",
    "    f = open(path,'r')\n",
    "    text = f.read()\n",
    "    examples = [example.split(' ') for example in text.split('\\n')[:-1]]\n",
    "    if labelled:\n",
    "        labels = [int(line[0]) for line in examples]\n",
    "        data = [line[1:] for line in examples]\n",
    "        return data,np.array(labels)\n",
    "    else:\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_unlabelled(model_2, unlabelled_it, 'predictions_twoFINAL.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_unlabelled_rnn(model, it, name):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        f = open(name, 'w')\n",
    "        for batch in it:\n",
    "            outputs = model(batch.Text).squeeze(1)\n",
    "            s = torch.sigmoid(outputs)\n",
    "            preds = torch.round(s)\n",
    "            output_string = '\\n'.join(str(int(s)) for s in preds.numpy())\n",
    "            f.write(output_string+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_unlabelled(model_2, unlabelled_it, 'predictions_q2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_3():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    train_it, val_it, test_it = data.BucketIterator.splits(\n",
    "            datasets=(train_set, val_set, test_set),\n",
    "            batch_size=4, device=device,\n",
    "            sort_key=lambda x: len(x.Text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "    \n",
    "    ## define model\n",
    "    model = EmbeddingClassifier(100, 2, len(TEXT.vocab), pretrained_vocab=TEXT.vocab)\n",
    "#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(25):\n",
    "        train_loss, train_acc = train(model, train_it, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_it, criterion)\n",
    "\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_it, criterion)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.537 | Train Acc: 80.76% | Val. Loss: 0.444 | Val. Acc: 88.53% |\n",
      "| Epoch: 02 | Train Loss: 0.444 | Train Acc: 89.55% | Val. Loss: 0.409 | Val. Acc: 90.94% |\n",
      "| Epoch: 03 | Train Loss: 0.415 | Train Acc: 91.71% | Val. Loss: 0.393 | Val. Acc: 92.44% |\n",
      "| Epoch: 04 | Train Loss: 0.401 | Train Acc: 92.62% | Val. Loss: 0.385 | Val. Acc: 93.14% |\n",
      "| Epoch: 05 | Train Loss: 0.391 | Train Acc: 93.39% | Val. Loss: 0.378 | Val. Acc: 93.78% |\n",
      "| Epoch: 06 | Train Loss: 0.385 | Train Acc: 93.90% | Val. Loss: 0.374 | Val. Acc: 94.26% |\n",
      "| Epoch: 07 | Train Loss: 0.380 | Train Acc: 94.33% | Val. Loss: 0.371 | Val. Acc: 94.38% |\n",
      "| Epoch: 08 | Train Loss: 0.376 | Train Acc: 94.62% | Val. Loss: 0.369 | Val. Acc: 94.61% |\n",
      "| Epoch: 09 | Train Loss: 0.373 | Train Acc: 94.92% | Val. Loss: 0.367 | Val. Acc: 94.79% |\n",
      "| Epoch: 10 | Train Loss: 0.370 | Train Acc: 95.14% | Val. Loss: 0.366 | Val. Acc: 94.80% |\n",
      "| Epoch: 11 | Train Loss: 0.367 | Train Acc: 95.34% | Val. Loss: 0.364 | Val. Acc: 94.95% |\n",
      "| Epoch: 12 | Train Loss: 0.366 | Train Acc: 95.53% | Val. Loss: 0.363 | Val. Acc: 95.02% |\n",
      "| Epoch: 13 | Train Loss: 0.364 | Train Acc: 95.69% | Val. Loss: 0.362 | Val. Acc: 95.29% |\n",
      "| Epoch: 14 | Train Loss: 0.362 | Train Acc: 95.82% | Val. Loss: 0.361 | Val. Acc: 95.29% |\n",
      "| Epoch: 15 | Train Loss: 0.360 | Train Acc: 95.98% | Val. Loss: 0.360 | Val. Acc: 95.35% |\n",
      "| Epoch: 16 | Train Loss: 0.359 | Train Acc: 96.08% | Val. Loss: 0.360 | Val. Acc: 95.45% |\n",
      "| Epoch: 17 | Train Loss: 0.358 | Train Acc: 96.19% | Val. Loss: 0.359 | Val. Acc: 95.52% |\n",
      "| Epoch: 18 | Train Loss: 0.357 | Train Acc: 96.30% | Val. Loss: 0.359 | Val. Acc: 95.46% |\n",
      "| Epoch: 19 | Train Loss: 0.356 | Train Acc: 96.45% | Val. Loss: 0.358 | Val. Acc: 95.62% |\n",
      "| Epoch: 20 | Train Loss: 0.355 | Train Acc: 96.50% | Val. Loss: 0.357 | Val. Acc: 95.68% |\n",
      "| Epoch: 21 | Train Loss: 0.354 | Train Acc: 96.58% | Val. Loss: 0.357 | Val. Acc: 95.64% |\n",
      "| Epoch: 22 | Train Loss: 0.353 | Train Acc: 96.73% | Val. Loss: 0.357 | Val. Acc: 95.54% |\n",
      "| Epoch: 23 | Train Loss: 0.352 | Train Acc: 96.73% | Val. Loss: 0.357 | Val. Acc: 95.64% |\n",
      "| Epoch: 24 | Train Loss: 0.352 | Train Acc: 96.80% | Val. Loss: 0.356 | Val. Acc: 95.56% |\n",
      "| Epoch: 25 | Train Loss: 0.351 | Train Acc: 96.78% | Val. Loss: 0.356 | Val. Acc: 95.68% |\n"
     ]
    }
   ],
   "source": [
    "model_3 = run_model_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9518\n"
     ]
    }
   ],
   "source": [
    "predict_unlabelled(model_3, test_test_it, 'TEST_OUT.txt')\n",
    "_,test_labels = read_data('../data/test.txt')\n",
    "f = open('TEST_OUT.txt','r')\n",
    "text = f.read()\n",
    "saved_data = [int(c) for c in text.split('\\n')[:-1]]\n",
    "correct = np.array(saved_data) == test_labels\n",
    "print(np.sum(correct)/correct.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_unlabelled(model_3, unlabelled_it, 'predictions_qTHREE_FINAL.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_labels, hidden_dim, vocab_size, pretrained_vocab=None):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        \n",
    "        if pretrained_vocab:\n",
    "            self.embedding.weight.data.copy_(pretrained_vocab.vectors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        z1 = self.embedding(x)        \n",
    "        z2, h2 = self.rnn(z1)\n",
    "        z3 = self.linear(h2.squeeze(0))\n",
    "        \n",
    "        return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_4():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    train_it, val_it, test_it = data.BucketIterator.splits(\n",
    "            datasets=(train_set, val_set, test_set),\n",
    "            batch_size=64, device=device,\n",
    "            sort_key=lambda x: len(x.Text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "    \n",
    "    ## define model\n",
    "    model = RNNClassifier(100, 1, 300, len(TEXT.vocab), pretrained_vocab=TEXT.vocab)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "        \n",
    "    for epoch in range(5):\n",
    "        train_loss, train_acc = train_rnn(model, train_it, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate_rnn(model, val_it, criterion)\n",
    "\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "\n",
    "    test_loss, test_acc = evaluate_rnn(model, test_it, criterion)\n",
    "    print('Test info:', test_loss, test_acc)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.300 | Train Acc: 87.02% | Val. Loss: 0.154 | Val. Acc: 94.76% |\n",
      "| Epoch: 02 | Train Loss: 0.149 | Train Acc: 94.98% | Val. Loss: 0.159 | Val. Acc: 94.31% |\n",
      "| Epoch: 03 | Train Loss: 0.120 | Train Acc: 96.10% | Val. Loss: 0.124 | Val. Acc: 95.66% |\n",
      "| Epoch: 04 | Train Loss: 0.117 | Train Acc: 96.20% | Val. Loss: 0.187 | Val. Acc: 93.30% |\n",
      "| Epoch: 05 | Train Loss: 0.100 | Train Acc: 96.83% | Val. Loss: 0.144 | Val. Acc: 95.23% |\n",
      "Test info: 0.14661894242759724 0.9522292993630573\n"
     ]
    }
   ],
   "source": [
    "model_4 = run_model_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9518\n"
     ]
    }
   ],
   "source": [
    "predict_unlabelled_rnn(model_4, test_test_it, 'TEST_OUT.txt')\n",
    "_,test_labels = read_data('../data/test.txt')\n",
    "f = open('TEST_OUT.txt','r')\n",
    "text = f.read()\n",
    "saved_data = [int(c) for c in text.split('\\n')[:-1]]\n",
    "correct = np.array(saved_data) == test_labels\n",
    "print(np.sum(correct)/correct.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_unlabelled_rnn(model_4, unlabelled_it, 'predictions_qFOUR_FINAL.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_labels, hidden_dim, vocab_size, pretrained_vocab=None):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        \n",
    "        if pretrained_vocab:\n",
    "            self.embedding.weight.data.copy_(pretrained_vocab.vectors)\n",
    "\n",
    "    def forward(self, x, batch_size=None):\n",
    "        z1 = self.embedding(x)\n",
    "        z2, (h2, c2) = self.lstm(z1)\n",
    "        z3 = self.linear(h2[-1])\n",
    "\n",
    "        return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_5():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    train_it, val_it, test_it = data.BucketIterator.splits(\n",
    "            datasets=(train_set, val_set, test_set),\n",
    "            batch_size=64, device=device,\n",
    "            sort_key=lambda x: len(x.Text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "    \n",
    "    ## define model\n",
    "    model = LSTMClassifier(100, 1, 300, len(TEXT.vocab), pretrained_vocab=TEXT.vocab)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "        \n",
    "    for epoch in range(5):\n",
    "        train_loss, train_acc = train_rnn(model, train_it, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate_rnn(model, val_it, criterion)\n",
    "\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "\n",
    "    test_loss, test_acc = evaluate_rnn(model, test_it, criterion)\n",
    "    print('Test info:', test_loss, test_acc)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.227 | Train Acc: 90.08% | Val. Loss: 0.180 | Val. Acc: 95.94% |\n",
      "| Epoch: 02 | Train Loss: 0.098 | Train Acc: 96.66% | Val. Loss: 0.118 | Val. Acc: 96.44% |\n",
      "| Epoch: 03 | Train Loss: 0.070 | Train Acc: 97.65% | Val. Loss: 0.111 | Val. Acc: 96.32% |\n",
      "| Epoch: 04 | Train Loss: 0.050 | Train Acc: 98.37% | Val. Loss: 0.105 | Val. Acc: 96.57% |\n",
      "| Epoch: 05 | Train Loss: 0.040 | Train Acc: 98.71% | Val. Loss: 0.101 | Val. Acc: 96.88% |\n",
      "Test info: 0.11581412699239649 0.9619824840764332\n"
     ]
    }
   ],
   "source": [
    "model_5 = run_model_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9622\n"
     ]
    }
   ],
   "source": [
    "predict_unlabelled_rnn(model_5, test_test_it, 'TEST_OUT.txt')\n",
    "_,test_labels = read_data('../data/test.txt')\n",
    "f = open('TEST_OUT.txt','r')\n",
    "text = f.read()\n",
    "saved_data = [int(c) for c in text.split('\\n')[:-1]]\n",
    "correct = np.array(saved_data) == test_labels\n",
    "print(np.sum(correct)/correct.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_unlabelled_rnn(model_5, unlabelled_it, 'predictions_qFIVE_FINAL.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
