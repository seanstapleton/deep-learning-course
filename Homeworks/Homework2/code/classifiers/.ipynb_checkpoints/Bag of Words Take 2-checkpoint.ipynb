{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5434 Acc: 0.8133\n",
      "val Loss: 0.4931 Acc: 0.8586\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4790 Acc: 0.8751\n",
      "val Loss: 0.4637 Acc: 0.8869\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.4565 Acc: 0.8933\n",
      "val Loss: 0.4483 Acc: 0.8965\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.4432 Acc: 0.9044\n",
      "val Loss: 0.4379 Acc: 0.9041\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4340 Acc: 0.9114\n",
      "val Loss: 0.4305 Acc: 0.9115\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.4271 Acc: 0.9162\n",
      "val Loss: 0.4248 Acc: 0.9137\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.4215 Acc: 0.9207\n",
      "val Loss: 0.4201 Acc: 0.9182\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.4186 Acc: 0.9226\n",
      "val Loss: 0.4199 Acc: 0.9182\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.4183 Acc: 0.9229\n",
      "val Loss: 0.4196 Acc: 0.9185\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.4181 Acc: 0.9231\n",
      "val Loss: 0.4194 Acc: 0.9186\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.4179 Acc: 0.9232\n",
      "val Loss: 0.4192 Acc: 0.9185\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.4177 Acc: 0.9235\n",
      "val Loss: 0.4190 Acc: 0.9185\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.4175 Acc: 0.9235\n",
      "val Loss: 0.4188 Acc: 0.9190\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.4173 Acc: 0.9236\n",
      "val Loss: 0.4186 Acc: 0.9187\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9237\n",
      "val Loss: 0.4186 Acc: 0.9187\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9237\n",
      "val Loss: 0.4186 Acc: 0.9188\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9237\n",
      "val Loss: 0.4186 Acc: 0.9188\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9237\n",
      "val Loss: 0.4186 Acc: 0.9188\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9237\n",
      "val Loss: 0.4186 Acc: 0.9188\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9237\n",
      "val Loss: 0.4186 Acc: 0.9189\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9238\n",
      "val Loss: 0.4186 Acc: 0.9189\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9238\n",
      "val Loss: 0.4186 Acc: 0.9189\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9238\n",
      "val Loss: 0.4186 Acc: 0.9189\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9238\n",
      "val Loss: 0.4186 Acc: 0.9189\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.4171 Acc: 0.9238\n",
      "val Loss: 0.4186 Acc: 0.9189\n",
      "Training complete in 3m 27s\n",
      "Best val Acc: 0.919000\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 0.919000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from dataset import ReviewsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from bow import BagOfWordsClassifier\n",
    "import pickle\n",
    "\n",
    "def train_model(device, dataloaders, dataset_sizes, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "   \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # If there is no training happening\n",
    "    if num_epochs == 0:\n",
    "        model.eval()\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # statistics\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        best_acc = running_corrects.double() / dataset_sizes['val']\n",
    "\n",
    "    # Training for num_epochs steps\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    ####################################################################################\n",
    "                    #                             END OF YOUR CODE                                     #\n",
    "                    ####################################################################################\n",
    "                    \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                with open('model.pkl', 'wb') as f:\n",
    "                    pickle.dump(best_model_wts, f)\n",
    "\n",
    "                \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    if num_epochs > 0:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def train(device, dataloaders, dataset_sizes, vocab_size):\n",
    "    model = BagOfWordsClassifier(2, vocab_size)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.05)\n",
    "\n",
    "    # Train the model for 25 epochs\n",
    "    print('Train the model')\n",
    "    model = train_model(device, dataloaders, dataset_sizes, model, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n",
    "    \n",
    "    train_model(device, dataloaders, dataset_sizes, model, criterion, optimizer, exp_lr_scheduler, num_epochs=0)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    datasets = {}\n",
    "    datasets['train'] = ReviewsDataset('../data/train.txt')\n",
    "    datasets['val'] = ReviewsDataset('../data/dev.txt', train=False, word2idx=datasets['train'].word2idx)\n",
    "    \n",
    "    dataset_sizes = { x: len(datasets[x]) for x in ['train', 'val'] }\n",
    "\n",
    "    vocab_size = len(datasets['train'].word2idx)\n",
    "\n",
    "    dataloaders = {\n",
    "        x: DataLoader(datasets[x], batch_size=5, shuffle=True, num_workers=4) for x in ['train', 'val']\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = train(device, dataloaders, dataset_sizes, vocab_size)\n",
    "    return model\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    model = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfWordsClassifier(\n",
       "  (linear): Linear(in_features=7639, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "datasets['train'] = ReviewsDataset('../data/train.txt')\n",
    "datasets['val'] = ReviewsDataset('../data/dev.txt', train=False, word2idx=datasets['train'].word2idx)\n",
    "\n",
    "dataset_sizes = { x: len(datasets[x]) for x in ['train', 'val'] }\n",
    "\n",
    "vocab_size = len(datasets['train'].word2idx)\n",
    "\n",
    "dataloaders = {\n",
    "    x: DataLoader(datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(datasets['val'].data)\n",
    "_,preds = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.919"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds.numpy() == datasets['val'].labels.numpy())/len(datasets['val'].labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ReviewsDataset('../data/test.txt', train=False, word2idx=datasets['train'].word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(test.data)\n",
    "_,preds = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9197"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds.numpy() == test.labels.numpy())/len(test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def read_data(path, labelled=True):\n",
    "    f = open(path,'r')\n",
    "    text = f.read()\n",
    "    examples = [example.split(' ') for example in text.split('\\n')[:-1]]\n",
    "    if labelled:\n",
    "        labels = [int(line[0]) for line in examples]\n",
    "        data = [line[1:] for line in examples]\n",
    "        return data,np.array(labels)\n",
    "    else:\n",
    "        return examples\n",
    "\n",
    "def create_vocab(data):\n",
    "    flatten = [w for line in data for w in line]\n",
    "    unique = list(set(flatten))\n",
    "    word2idx = {word: idx for idx,word in enumerate(unique)}\n",
    "    return unique,word2idx\n",
    "\n",
    "def create_bag_of_words(data, word2idx=None):\n",
    "    if word2idx is None:\n",
    "        raise Error('create_bag_of_words need a word2idx mapping!')\n",
    "    bag_of_words = np.zeros((len(data), len(word2idx)))\n",
    "    for line in range(len(data)):\n",
    "        for word in data[line]:\n",
    "            if word in word2idx:\n",
    "                bag_of_words[line][word2idx[word]] += 1\n",
    "    return bag_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_data(path)\n",
    "\n",
    "if train:\n",
    "    vocab,word2idx = create_vocab(data)\n",
    "elif word2idx is None:\n",
    "    raise Error('Vocab must be provided for non-training data in ReviewsDataset')\n",
    "\n",
    "self.word2idx = word2idx\n",
    "\n",
    "data = create_bag_of_words(data, word2idx=word2idx)\n",
    "self.data = torch.tensor(data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ReviewsDataset('../data/unlabelled.txt', train=False, word2idx=datasets['train'].word2idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
